from feature import all_model_parm
import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.subplots(2,2)

ax[0, 0].plot(range(1, 6), all_model_parm[0][0])
ax[0, 0].plot(range(1, 6), all_model_parm[1][0])
ax[0, 0].plot(range(1, 6), all_model_parm[2][0])
ax[0, 0].plot(range(1, 6), all_model_parm[3][0])
ax[0, 0].plot(range(1, 6), all_model_parm[4][0])
ax[0, 0].plot(range(1, 6), all_model_parm[5][0])
ax[0, 0].plot(range(1, 6), all_model_parm[6][0])

ax[0, 1].plot(range(1,6), all_model_parm[0][1])
ax[0, 1].plot(range(1,6), all_model_parm[1][1])
ax[0, 1].plot(range(1,6), all_model_parm[2][1])
ax[0, 1].plot(range(1,6), all_model_parm[3][1])
ax[0, 1].plot(range(1,6), all_model_parm[4][1])
ax[0, 1].plot(range(1,6), all_model_parm[5][1])
ax[0, 1].plot(range(1,6), all_model_parm[6][1])

ax[1, 0].plot(range(1,6), all_model_parm[0][2])
ax[1, 0].plot(range(1,6), all_model_parm[1][2])
ax[1, 0].plot(range(1,6), all_model_parm[2][2])
ax[1, 0].plot(range(1,6), all_model_parm[3][2])
ax[1, 0].plot(range(1,6), all_model_parm[4][2])
ax[1, 0].plot(range(1,6), all_model_parm[5][2])
ax[1, 0].plot(range(1,6), all_model_parm[6][2])

ax[1, 1].plot(range(1,6), all_model_parm[0][3])
ax[1, 1].plot(range(1,6), all_model_parm[1][3])
ax[1, 1].plot(range(1,6), all_model_parm[2][3])
ax[1, 1].plot(range(1,6), all_model_parm[3][3])
ax[1, 1].plot(range(1,6), all_model_parm[4][3])
ax[1, 1].plot(range(1,6), all_model_parm[5][3])
plt.ylim(0.5, 1)
fig.show()
plt.figure(figsize=(10, 10))
plt.subplot(221)
plt.title("accuracy")
#plt.plot(range(1,6), all_model_parm[0][0], 'r-^', label='KNN')
#plt.plot(range(1,6), all_model_parm[1][0], 'k-^', label='Bayes')
#plt.plot(range(1,6), all_model_parm[2][0], 'y-^', label='DTree')
#plt.plot(range(1,6), all_model_parm[3][0], 'g-^', label='LR')
plt.plot(range(1,6), all_model_parm[4][0], 'y-^', label='RF')
plt.plot(range(1,6), all_model_parm[5][0], 'b-^', label='SVM')
plt.plot(range(1,6), all_model_parm[6][0], 'g-^', label='XGBoost')
#plt.plot(range(1,6), all_model_parm[7][0], label='MLP', color="orange", linestyle='-', marker='^')
plt.ylim(0.5, 1)
plt.subplot(222)
plt.title("recall")
#plt.plot(range(1,6), all_model_parm[0][1], 'r-^', label='KNN')
#plt.plot(range(1,6), all_model_parm[1][1], 'k-^', label='Bayes')
#plt.plot(range(1,6), all_model_parm[2][1], 'y-^', label='Dtree')
#plt.plot(range(1,6), all_model_parm[3][1], 'g-^', label='LR')
plt.plot(range(1,6), all_model_parm[4][1], 'y-^', label='RF')
plt.plot(range(1,6), all_model_parm[5][1], 'b-^', label='SVM')
plt.plot(range(1,6), all_model_parm[6][1], 'g-^', label='XGBoost')
#plt.plot(range(1,6), all_model_parm[7][1], label='MLP', color="orange", linestyle='-', marker='^')

plt.ylim(0.5, 1)
plt.subplot(223)
plt.title("precision")
#plt.plot(range(1,6), all_model_parm[0][2], 'r-^', label='KNN')
#plt.plot(range(1,6), all_model_parm[1][2], 'k-^', label='Bayes')
#plt.plot(range(1,6), all_model_parm[2][2], 'y-^', label='Dtree')
#plt.plot(range(1,6), all_model_parm[3][2], 'g-^', label='LR')
plt.plot(range(1,6), all_model_parm[4][2], 'y-^', label='RF')
plt.plot(range(1,6), all_model_parm[5][2], 'b-^', label='SVM')
plt.plot(range(1,6), all_model_parm[6][2], 'g-^', label='XGBoost')
#plt.plot(range(1,6), all_model_parm[7][2], label='MLP', color="orange", linestyle='-', marker='^')

plt.ylim(0.5, 1)
plt.subplot(224)
plt.title("f1")
#plt.plot(range(1,6), all_model_parm[0][3], 'r-^', label='KNN')
#plt.plot(range(1,6), all_model_parm[1][3], 'k-^', label='Bayes')
#plt.plot(range(1,6), all_model_parm[2][3], 'y-^', label='Dtree')
#plt.plot(range(1,6), all_model_parm[3][3], 'g-^', label='LR')
plt.plot(range(1,6), all_model_parm[4][3], 'y-^', label='RF')
plt.plot(range(1,6), all_model_parm[5][3], 'b-^', label='SVM')
plt.plot(range(1,6), all_model_parm[6][3], 'g-^', label='XGBoost')
#plt.plot(range(1,6), all_model_parm[7][3], label='MLP', color="orange", linestyle='-', marker='^')

plt.ylim(0.5, 1)
plt.show()
